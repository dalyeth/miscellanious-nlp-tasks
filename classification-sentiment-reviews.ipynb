{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8044463,"sourceType":"datasetVersion","datasetId":4743243}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Задача: продемонстрировать различные подходы  к решению задачи классификации на основе выборки с отзывами ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pymorphy2","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:07:44.626030Z","iopub.execute_input":"2024-04-07T09:07:44.626728Z","iopub.status.idle":"2024-04-07T09:08:00.274448Z","shell.execute_reply.started":"2024-04-07T09:07:44.626667Z","shell.execute_reply":"2024-04-07T09:08:00.273441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pymorphy2\nfrom pymorphy2 import MorphAnalyzer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam, AdamW\n\nfrom transformers import AutoModel, AutoTokenizer, BertForSequenceClassification,AutoModelForSequenceClassification, BertTokenizerFast, get_linear_schedule_with_warmup, AutoConfig\n\nfrom typing import Dict\nfrom numpy import asarray\nfrom functools import reduce\nfrom tqdm.notebook import tqdm\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, f1_score\n\ntorch.manual_seed(42)\n\nimport nltk\n\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom nltk.corpus import stopwords\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:17:53.183674Z","iopub.execute_input":"2024-04-07T09:17:53.184044Z","iopub.status.idle":"2024-04-07T09:18:02.485461Z","shell.execute_reply.started":"2024-04-07T09:17:53.184014Z","shell.execute_reply":"2024-04-07T09:18:02.484557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/testtaskreviews/DS_task_NLP_20240406/data.xlsx')\ntest = pd.read_excel('/kaggle/input/testtaskreviews/DS_task_NLP_20240406/test.xlsx')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:18:38.143200Z","iopub.execute_input":"2024-04-07T09:18:38.143558Z","iopub.status.idle":"2024-04-07T09:18:39.100417Z","shell.execute_reply.started":"2024-04-07T09:18:38.143530Z","shell.execute_reply":"2024-04-07T09:18:39.099406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ndf.sentiment =  le.fit_transform(df.sentiment)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:18:41.267436Z","iopub.execute_input":"2024-04-07T09:18:41.268294Z","iopub.status.idle":"2024-04-07T09:18:41.277019Z","shell.execute_reply.started":"2024-04-07T09:18:41.268259Z","shell.execute_reply":"2024-04-07T09:18:41.275913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:34:42.337000Z","iopub.execute_input":"2024-04-07T08:34:42.337478Z","iopub.status.idle":"2024-04-07T08:34:42.357661Z","shell.execute_reply.started":"2024-04-07T08:34:42.337446Z","shell.execute_reply":"2024-04-07T08:34:42.356048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:34:46.474122Z","iopub.execute_input":"2024-04-07T08:34:46.475163Z","iopub.status.idle":"2024-04-07T08:34:46.491349Z","shell.execute_reply.started":"2024-04-07T08:34:46.475080Z","shell.execute_reply":"2024-04-07T08:34:46.490216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npatterns = \"[0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\nmorph = MorphAnalyzer()\n\ndef lemmatize(doc):\n    doc = re.sub(patterns, ' ', doc)\n    tokens = []\n    for token in doc.split():\n        token = token.strip()\n        token_norm = morph.normal_forms(token)[0]\n        \n        if token_norm:\n            tokens.append(token_norm.lower())                                \n        else: \n            tokens.append(token)\n    return ' '.join(tokens) \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:34:52.766806Z","iopub.execute_input":"2024-04-07T08:34:52.767231Z","iopub.status.idle":"2024-04-07T08:34:53.318609Z","shell.execute_reply.started":"2024-04-07T08:34:52.767200Z","shell.execute_reply":"2024-04-07T08:34:53.317244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lem'] = df['review'].apply(lemmatize)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:34:56.009918Z","iopub.execute_input":"2024-04-07T08:34:56.010399Z","iopub.status.idle":"2024-04-07T08:35:34.160191Z","shell.execute_reply.started":"2024-04-07T08:34:56.010366Z","shell.execute_reply":"2024-04-07T08:35:34.158772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lem'].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:35:34.162621Z","iopub.execute_input":"2024-04-07T08:35:34.162981Z","iopub.status.idle":"2024-04-07T08:35:34.174301Z","shell.execute_reply.started":"2024-04-07T08:35:34.162951Z","shell.execute_reply":"2024-04-07T08:35:34.172614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(20)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:35:34.175925Z","iopub.execute_input":"2024-04-07T08:35:34.176256Z","iopub.status.idle":"2024-04-07T08:35:34.196727Z","shell.execute_reply.started":"2024-04-07T08:35:34.176229Z","shell.execute_reply":"2024-04-07T08:35:34.195286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf = TfidfVectorizer(\n    lowercase=True,\n    analyzer=\"word\",    \n    ngram_range=(1, 3),\n    dtype=np.float32\n    \n) ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:35:34.199792Z","iopub.execute_input":"2024-04-07T08:35:34.200211Z","iopub.status.idle":"2024-04-07T08:35:34.206825Z","shell.execute_reply.started":"2024-04-07T08:35:34.200179Z","shell.execute_reply":"2024-04-07T08:35:34.205337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = train_test_split(df, stratify = df['sentiment'], test_size = 0.15, random_state =112)\ntrain.shape, val.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:18:50.425844Z","iopub.execute_input":"2024-04-07T09:18:50.426172Z","iopub.status.idle":"2024-04-07T09:18:50.444807Z","shell.execute_reply.started":"2024-04-07T09:18:50.426148Z","shell.execute_reply":"2024-04-07T09:18:50.443903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = tf_idf.fit_transform(train['lem'])\nx_val = tf_idf.transform(val['lem'])\ny_train = train['sentiment']\ny_val = val['sentiment']","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:35:34.238937Z","iopub.execute_input":"2024-04-07T08:35:34.239395Z","iopub.status.idle":"2024-04-07T08:35:35.399787Z","shell.execute_reply.started":"2024-04-07T08:35:34.239360Z","shell.execute_reply":"2024-04-07T08:35:35.398624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, x, y): \n    preds = model.predict(x)\n    print(classification_report(y, preds))\n    return f1_score(y, preds, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:36:43.811482Z","iopub.execute_input":"2024-04-07T08:36:43.811939Z","iopub.status.idle":"2024-04-07T08:36:43.818192Z","shell.execute_reply.started":"2024-04-07T08:36:43.811908Z","shell.execute_reply":"2024-04-07T08:36:43.817090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegressionCV(max_iter=1000, scoring='f1_weighted')\nlogreg.fit(x_train, y_train)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(logreg, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:52:24.831543Z","iopub.execute_input":"2024-04-07T08:52:24.832337Z","iopub.status.idle":"2024-04-07T08:52:24.856497Z","shell.execute_reply.started":"2024-04-07T08:52:24.832297Z","shell.execute_reply":"2024-04-07T08:52:24.855314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Взвешенные эмбеддинги fasttext + логистическая регрессия\n","metadata":{}},{"cell_type":"code","source":"!pip install fasttext","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:52:34.542659Z","iopub.execute_input":"2024-04-07T08:52:34.543112Z","iopub.status.idle":"2024-04-07T08:52:50.166630Z","shell.execute_reply.started":"2024-04-07T08:52:34.543081Z","shell.execute_reply":"2024-04-07T08:52:50.165121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\nfrom huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ru-vectors\", filename=\"model.bin\")\nmodel_ft = fasttext.load_model(model_path)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:52:50.171791Z","iopub.execute_input":"2024-04-07T08:52:50.172396Z","iopub.status.idle":"2024-04-07T08:56:12.333556Z","shell.execute_reply.started":"2024-04-07T08:52:50.172348Z","shell.execute_reply":"2024-04-07T08:56:12.330593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_dict = dict(zip(tf_idf.get_feature_names_out(), tf_idf.idf_))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:56:12.337977Z","iopub.execute_input":"2024-04-07T08:56:12.338452Z","iopub.status.idle":"2024-04-07T08:56:12.558942Z","shell.execute_reply.started":"2024-04-07T08:56:12.338417Z","shell.execute_reply":"2024-04-07T08:56:12.557522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def emb_weigh(text):\n    text = text.split()\n    if len(text) > 0:  \n        tokens = [model_ft.get_word_vector(word)* tfidf_dict[word] if word in tfidf_dict else model_ft.get_word_vector(word)*tf_idf.idf_.max() for word in text ]\n        summed_vecs =  reduce((lambda x,y: x+y), tokens)\n        return summed_vecs / len(text) \n    else:\n        return np.zeros(300)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:56:12.560554Z","iopub.execute_input":"2024-04-07T08:56:12.561327Z","iopub.status.idle":"2024-04-07T08:56:12.964873Z","shell.execute_reply.started":"2024-04-07T08:56:12.561292Z","shell.execute_reply":"2024-04-07T08:56:12.963562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train['lem'].apply(emb_weigh).to_list()\nx_val = val['lem'].apply(emb_weigh).to_list()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:59:45.641081Z","iopub.execute_input":"2024-04-07T08:59:45.641669Z","iopub.status.idle":"2024-04-07T08:59:59.311762Z","shell.execute_reply.started":"2024-04-07T08:59:45.641629Z","shell.execute_reply":"2024-04-07T08:59:59.310148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_ft  = LogisticRegressionCV(max_iter=1000, scoring='f1_weighted')\nlr_ft.fit(x_train, y_train)\nevaluate(lr_ft, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:59:59.314482Z","iopub.execute_input":"2024-04-07T08:59:59.314885Z","iopub.status.idle":"2024-04-07T09:01:07.223122Z","shell.execute_reply.started":"2024-04-07T08:59:59.314852Z","shell.execute_reply":"2024-04-07T09:01:07.221792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Эмбеддинги предложений из предобученного берта  + логистическая регрессия\n","metadata":{}},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:01:31.033695Z","iopub.execute_input":"2024-04-07T09:01:31.034217Z","iopub.status.idle":"2024-04-07T09:01:47.164475Z","shell.execute_reply.started":"2024-04-07T09:01:31.034179Z","shell.execute_reply":"2024-04-07T09:01:47.162006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nsbert = SentenceTransformer('cointegrated/rubert-tiny2')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:01:56.840921Z","iopub.execute_input":"2024-04-07T09:01:56.841441Z","iopub.status.idle":"2024-04-07T09:02:01.679559Z","shell.execute_reply.started":"2024-04-07T09:01:56.841403Z","shell.execute_reply":"2024-04-07T09:02:01.677748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nx_train = train['review'].apply(lambda x: sbert.encode(x, show_progress_bar = False)).to_list()\nx_val = val['review'].apply(lambda x: sbert.encode(x, show_progress_bar = False)).to_list()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:02:11.723839Z","iopub.execute_input":"2024-04-07T09:02:11.724333Z","iopub.status.idle":"2024-04-07T09:03:02.114675Z","shell.execute_reply.started":"2024-04-07T09:02:11.724298Z","shell.execute_reply":"2024-04-07T09:03:02.113690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_bert = LogisticRegressionCV(max_iter = 3000, scoring = 'f1_weighted')\nlr_bert.fit(x_train, y_train)\nevaluate(lr_bert, x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:03:02.116611Z","iopub.execute_input":"2024-04-07T09:03:02.117598Z","iopub.status.idle":"2024-04-07T09:04:17.177596Z","shell.execute_reply.started":"2024-04-07T09:03:02.117562Z","shell.execute_reply":"2024-04-07T09:04:17.176353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## токенизатор rubert-tiny2 + RNN","metadata":{}},{"cell_type":"code","source":"# токенизатор берта тут используется для простоты и с целью использования преимуществ токенизации трансформеров и уже готового словаря \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('seara/rubert-tiny2-russian-sentiment')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:18:05.468901Z","iopub.execute_input":"2024-04-07T09:18:05.469461Z","iopub.status.idle":"2024-04-07T09:18:06.275873Z","shell.execute_reply.started":"2024-04-07T09:18:05.469419Z","shell.execute_reply":"2024-04-07T09:18:06.274903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:18:10.381107Z","iopub.execute_input":"2024-04-07T09:18:10.381496Z","iopub.status.idle":"2024-04-07T09:18:10.440918Z","shell.execute_reply.started":"2024-04-07T09:18:10.381463Z","shell.execute_reply":"2024-04-07T09:18:10.439611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, texts, targets, tokenizer, max_len=50):\n        self.texts = list(texts)\n        if targets is not None: \n            self.targets = list(targets)\n        else:\n            self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        if self.targets:\n            targets = self.targets[idx]\n\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            truncation=True,\n            return_tensors='pt',\n        )\n        if self.targets :\n            return {\n               'input_ids': encoding['input_ids'].flatten(),\n               'attention_mask': encoding['attention_mask'].flatten(),\n               'targets': torch.tensor(targets, dtype=torch.long),\n           \n                    }\n        else: \n            return {\n             'input_ids': encoding['input_ids'].flatten(),\n             'attention_mask': encoding['attention_mask'].flatten()\n                    }","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:38:46.233994Z","iopub.execute_input":"2024-04-07T09:38:46.234251Z","iopub.status.idle":"2024-04-07T09:38:46.243238Z","shell.execute_reply.started":"2024-04-07T09:38:46.234230Z","shell.execute_reply":"2024-04-07T09:38:46.242291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset = CustomDataset(texts=train['review'], targets=train['sentiment'],tokenizer=tokenizer)\nval_dataset = CustomDataset(texts=val['review'], targets=val['sentiment'], tokenizer=tokenizer)\ntest_dataset = CustomDataset(texts=test['review'], targets=None, tokenizer=tokenizer)\n\ntrain_data = DataLoader(train_dataset, batch_size=216, shuffle=True)\nval_data =  DataLoader(val_dataset, batch_size=216, shuffle=False)\ntest_data = DataLoader(test_dataset, batch_size=1, shuffle=False)\n ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:38:49.704786Z","iopub.execute_input":"2024-04-07T09:38:49.705423Z","iopub.status.idle":"2024-04-07T09:38:49.713887Z","shell.execute_reply.started":"2024-04-07T09:38:49.705390Z","shell.execute_reply":"2024-04-07T09:38:49.712918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_SIZE = 83828 #rubert-tiny2 vocab size\nEMBEDDING_DIM = 2048\n\n\nclass LSTMClassifier(nn.Module):\n\n    def __init__(self,  n_classes = 3, vocab_size = VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, hidden_dim=500, n_layers=2, bidirectional=True\n     ):\n\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            n_layers,\n            bidirectional=bidirectional,\n            dropout = 0.3,\n            batch_first=True,\n        )\n        self.hidden_dim = hidden_dim\n        self.output_dim = n_classes\n        self.linear = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n        self.projection = nn.Linear(self.hidden_dim, self.output_dim)\n        self.func = nn.Tanh()\n        self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, inputs):\n\n        inputs = self.embedding(inputs)\n        outputs, (hidden, cell) = self.rnn(inputs)\n        outputs = torch.mean(outputs, dim=1)\n        outputs = self.dropout(self.linear(self.func(outputs)))\n        projection = self.projection(self.func(outputs))\n       \n        return projection\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:19:11.420847Z","iopub.execute_input":"2024-04-07T09:19:11.421581Z","iopub.status.idle":"2024-04-07T09:19:11.431164Z","shell.execute_reply.started":"2024-04-07T09:19:11.421544Z","shell.execute_reply":"2024-04-07T09:19:11.430011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(model, train_loader,  epochs):\n    \n    losses = []\n    predictions = []\n    model.to(device)\n    model.train()\n    \n    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n    optimizer = Adam(model.parameters(), lr=2e-5)\n    for data in train_loader:\n        \n        optimizer.zero_grad()\n        targets = data[\"targets\"]\n        targets = targets.to(device)\n        inputs = data['input_ids'].to(device)\n        \n        \n        outputs = model(inputs).to(device)\n        loss = loss_fn(outputs.to(torch.float32), targets)\n        loss.backward()\n        losses.append(loss.item())\n                      \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n       \n    train_loss = np.mean(losses)\n        \n    return train_loss\n    \ndef eval(model, valid_loader):\n    model.eval()\n    all_logits = []\n    all_preds = []\n    all_labels = []\n    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n    \n    with torch.no_grad():\n        for data in valid_loader:\n            targets = data[\"targets\"]\n            targets = targets.to(device)\n            inputs = data['input_ids'].to(device)\n        \n            outputs = model(inputs).to(device)\n            loss = loss_fn(outputs.to(torch.float32), targets)\n    \n            all_logits.append(outputs)\n            all_labels.append(targets)\n            all_preds.extend(outputs.argmax(1).tolist())\n        \n        all_labels = torch.cat(all_labels).to(device)\n        all_logits = torch.cat(all_logits).to(device)\n        loss = loss_fn(all_logits, all_labels).item()\n        score = f1_score(all_labels.cpu(), all_preds, average='weighted' )\n        \n        print(classification_report(all_labels.cpu(), all_preds, zero_division=0))\n    \n    return loss, score \n    \ndef train_model(model, train_loader, valid_loader, epochs):\n        \n        best_score = 0\n        for epoch in range(epochs):\n            print(f'Epoch {epoch + 1}/{epochs}')\n            train_loss = fit(model, train_loader, epochs)\n            print(f'Train loss {train_loss}')\n\n            val_loss, val_score = eval(model, valid_loader)\n            print(f'Val loss {val_loss} f1_weighted {val_score}')\n            print('-' * 10)\n            \n            if val_score > best_score: \n                best_score = val_score\n                torch.save(model.state_dict(), f'/kaggle/working/lstm.pt')\n            \n            \n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:19:14.372568Z","iopub.execute_input":"2024-04-07T09:19:14.372936Z","iopub.status.idle":"2024-04-07T09:19:14.388233Z","shell.execute_reply.started":"2024-04-07T09:19:14.372906Z","shell.execute_reply":"2024-04-07T09:19:14.387092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LSTMClassifier()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:19:20.147225Z","iopub.execute_input":"2024-04-07T09:19:20.147752Z","iopub.status.idle":"2024-04-07T09:19:22.093761Z","shell.execute_reply.started":"2024-04-07T09:19:20.147679Z","shell.execute_reply":"2024-04-07T09:19:22.092963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, train_data, val_data,  epochs=30\n           )","metadata":{"execution":{"iopub.status.busy":"2024-04-07T08:18:30.748442Z","iopub.execute_input":"2024-04-07T08:18:30.748805Z","iopub.status.idle":"2024-04-07T08:23:54.011150Z","shell.execute_reply.started":"2024-04-07T08:18:30.748779Z","shell.execute_reply":"2024-04-07T08:23:54.010107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuned Bert (with frozen encoder)","metadata":{}},{"cell_type":"code","source":"trainer_config = {\n\n'save_score': 0,\n'n_epochs': 50,\n'device': device,\n'verbose':True, \n\"weight_decay\": 1e-4,\n}\nclass BertTrainer:\n    def __init__(self, config: Dict):\n        self.config = config\n        self.n_epochs = config['n_epochs']\n        self.device = config['device']\n        self.model =  AutoModelForSequenceClassification.from_pretrained('seara/rubert-tiny2-russian-sentiment', return_dict=True)\n        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n        self.model.classifier = torch.nn.Linear(self.out_features, 3)\n        self.history = None\n        self.verbose = config.get('verbose', True)\n        self.best_score = config['save_score']\n    def fit(self, train_dataloader, val_dataloader, trainable=True):\n        best_score = self.best_score\n        self.train_loader = train_dataloader\n        self.optimizer = AdamW(self.model.parameters(), lr=2e-5)\n        self.scheduler = get_linear_schedule_with_warmup(\n            self.optimizer,\n            num_warmup_steps=0,\n            num_training_steps=len(self.train_loader) * self.n_epochs\n            )\n        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n        \n        \n        self.history = {\n            'train_loss': [],\n            'val_loss': [],\n            'val_fscore': []\n          }\n\n        for epoch in range(self.n_epochs):\n            print(f\"Epoch {epoch + 1}/{self.n_epochs}\")\n            train_info = self.train_epoch(train_dataloader)\n            val_info = self.val_epoch(val_dataloader)\n            self.history['train_loss'].extend([train_info['loss']])\n            self.history['val_loss'].extend([val_info['loss']])\n            self.history['val_fscore'].extend([val_info['fscore']])\n            if val_info['fscore'] > best_score: \n                best_score = val_info['fscore']\n                self.model.save_pretrained(f'bert_ft.pt')\n\n                \n        return self.model.eval()\n\n    def train_epoch(self, train_dataloader):\n        \n        self.model.to(self.device)\n        self.model = self.model.train()\n        losses = []\n        if self.verbose:\n            train_dataloader = tqdm(train_dataloader)\n        for batch in train_dataloader:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            targets = batch['targets'].to(self.device)\n            outputs = self.model(ids, mask)\n            loss = self.loss_fn(outputs.logits, targets)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()   \n            self.scheduler.step()\n            loss_val = loss.item()\n            losses.append(loss_val)\n        print('Train loss: ', np.mean(losses))\n        return {'loss': np.mean(losses)}\n\n    def val_epoch(self, val_dataloader):\n        self.model.eval()\n        all_logits = []\n        all_labels = []\n        all_preds = []\n        preds = []\n        if self.verbose:\n            val_dataloader = tqdm(val_dataloader)\n        with torch.no_grad():\n            for batch in val_dataloader:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                targets = batch['targets'].to(self.device)\n                outputs = self.model(ids, mask).logits\n                all_logits.append(outputs)\n                all_labels.append(targets)\n                all_preds.extend(outputs.argmax(1).tolist())\n        all_labels = torch.cat(all_labels).to(self.device)\n        \n        all_logits = torch.cat(all_logits).to(self.device)\n        loss = self.loss_fn(all_logits, all_labels).item()\n        report = classification_report(all_labels.cpu(), all_preds, zero_division=0)\n        fscore = f1_score(all_labels.cpu(), all_preds, average='weighted' )\n        print('Val loss:', loss)\n        print('F1_score: ', fscore)\n        print(report)\n            \n        return {\n            'fscore': fscore,\n            'loss': loss\n         }","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:39:04.837980Z","iopub.execute_input":"2024-04-07T09:39:04.838824Z","iopub.status.idle":"2024-04-07T09:39:04.860138Z","shell.execute_reply.started":"2024-04-07T09:39:04.838789Z","shell.execute_reply":"2024-04-07T09:39:04.859091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = BertTrainer(trainer_config)\n\nfor param in trainer.model.bert.parameters():\n    param.requires_grad=False\n    \ntrainer.fit(train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:20:17.769676Z","iopub.execute_input":"2024-04-07T09:20:17.770051Z","iopub.status.idle":"2024-04-07T09:22:13.402283Z","shell.execute_reply.started":"2024-04-07T09:20:17.770023Z","shell.execute_reply":"2024-04-07T09:22:13.401353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#попробуем немного подучить кодировщик поверх обученного классификатора\nfor param in trainer.model.bert.parameters():\n    param.requires_grad=True\ntrainer.n_epochs = 10\ntrainer.best_score = 0.767\ntrainer.fit(train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:24:29.295467Z","iopub.execute_input":"2024-04-07T09:24:29.296141Z","iopub.status.idle":"2024-04-07T09:24:59.523724Z","shell.execute_reply.started":"2024-04-07T09:24:29.296107Z","shell.execute_reply":"2024-04-07T09:24:59.522736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuned BERT (with encoder training)","metadata":{}},{"cell_type":"code","source":"#чуть настроим классификатор, чтобы не испортить веса на первых шагах\ntrainer_ = BertTrainer(trainer_config)\nfor param in trainer_.model.bert.parameters():\n    param.requires_grad=False\ntrainer_.n_epochs = 5\ntrainer_.best_score = 0.791 \ntrainer_.fit(train_data, val_data)\nfor param in trainer_.model.bert.parameters():\n    param.requires_grad=True\ntrainer_.n_epochs = 25\ntrainer_.fit(train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:39:56.496961Z","iopub.execute_input":"2024-04-07T09:39:56.497335Z","iopub.status.idle":"2024-04-07T09:41:20.493560Z","shell.execute_reply.started":"2024-04-07T09:39:56.497305Z","shell.execute_reply":"2024-04-07T09:41:20.492697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_model = AutoModelForSequenceClassification.from_pretrained('/kaggle/working/bert_ft.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:41:36.097787Z","iopub.execute_input":"2024-04-07T09:41:36.098157Z","iopub.status.idle":"2024-04-07T09:41:36.168290Z","shell.execute_reply.started":"2024-04-07T09:41:36.098128Z","shell.execute_reply":"2024-04-07T09:41:36.167467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_model.to(device)\nsaved_model.eval()\npreds = []\nwith torch.no_grad():\n    for batch in test_data:\n        ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        outputs = saved_model(ids, mask).logits\n        preds.extend(outputs.argmax(1).tolist())\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:44:01.827975Z","iopub.execute_input":"2024-04-07T09:44:01.828613Z","iopub.status.idle":"2024-04-07T09:44:05.376891Z","shell.execute_reply.started":"2024-04-07T09:44:01.828580Z","shell.execute_reply":"2024-04-07T09:44:05.375929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['predicted'] = preds\ntest['predicted'] = le.inverse_transform(test.predicted)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:45:20.430056Z","iopub.execute_input":"2024-04-07T09:45:20.430423Z","iopub.status.idle":"2024-04-07T09:45:20.437641Z","shell.execute_reply.started":"2024-04-07T09:45:20.430387Z","shell.execute_reply":"2024-04-07T09:45:20.436570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(20)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:46:29.356747Z","iopub.execute_input":"2024-04-07T09:46:29.357381Z","iopub.status.idle":"2024-04-07T09:46:29.368237Z","shell.execute_reply.started":"2024-04-07T09:46:29.357344Z","shell.execute_reply":"2024-04-07T09:46:29.367170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.to_csv('res.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T09:46:54.545709Z","iopub.execute_input":"2024-04-07T09:46:54.546377Z","iopub.status.idle":"2024-04-07T09:46:54.562433Z","shell.execute_reply.started":"2024-04-07T09:46:54.546344Z","shell.execute_reply":"2024-04-07T09:46:54.561667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Выводы: \nЦелью было не добиться максимальной достижимой метрики, а продемонстрировать разные подходы к задаче, поэтому препроцессинг базовый, а гиперпараметры не подбирались. \n\nБыли реализованы подходы: линейный классификтор на основе tf_idf, линейный классификатор с использованием взвешенных эмбеддингов fasttext, LSTM c токенизацией bert, обучение только классификтора без обучения кодировщика bert, дообучение кодировщика bert. \n\nДатасет изначально проблемный, т.к. помимо дисбаланса классов содержит отзывы как на русском, так и на английском. Если задача не ставится как мультиязычная, такие отзывы нужно вычищать. Соответственно, если ставится, то нужно ориентироваться на мультиязычные модели или строить разные пайплайны для обработки разных языков. Я не вычищала эти отзывы из обучающей выборки, т.к. тестовая тоже их содержит, и не пыталась реализовать разные пайлайны, т.к. это, кажется, чрезмерно в данной задаче. \n\nВ качестве метрики на несбалансированных данных используется f1_weighted\n\nВ качестве бейзлайна лучшую метрику показал fine-tuned Bert, при этом метрика без обучения энкодера лишь на несколько процентов уступает метрике, полученной с дообучением кодировщика. \n\nХороший бейзлайн дает tf_idf + логистическая регрессия. Варианты с использованием предобученных векторов проявили себя не очень, вероятно, из-за зашумленности данных англоязычными отзывами. \n\n  \n","metadata":{}}]}